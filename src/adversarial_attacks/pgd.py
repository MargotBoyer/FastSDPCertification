import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.autograd import grad


class PGDAttack:
    """
    Impléementation of the Projected Gradient Descent (PGD) attack.

    Args:
        model: the model to attack
        eps: pertubation ball radius
        alpha: width of the step size
        steps: number of steps to perform
        random_start: if True, starts with a random perturbation, else starts with zero perturbationœ
        targeted: if True, performs a targeted attack (attacks a specific class), else performs an untargeted attack
        norm: Types of norm ('inf', '2', '1')
    """

    def __init__(
        self,
        model,
        eps=0.3,
        alpha=0.01,
        steps=40,
        random_start=True,
        targeted=False,
        norm="inf",
    ):
        self.model = model
        self.eps = eps
        self.alpha = alpha
        self.steps = steps
        self.random_start = random_start
        self.targeted = targeted
        self.norm = norm

    def forward(self, images, labels, target_labels=None):
        """
        Generate adversarial images using the PGD attack.

        Args:
            images: Batch of images (torch.Tensor)
            labels: True labels of the images (torch.Tensor)
            target_labels: Target labels for targeted attacks (torch.Tensor, optional)

        Returns:
            adversarial_images: Adversarial images generated by the attack (torch.Tensor)
        """
        images = images.clone().detach()
        labels = labels.clone().detach()

        # Initialisation aléatoire si demandée
        if self.random_start:
            delta = self._random_init(images)
        else:
            delta = torch.zeros_like(images)

        delta.requires_grad = True

        for step in range(self.steps):
            # Forward pass
            adv_images = images + delta
            print("images shape:", adv_images.shape)
            print("model :", self.model)
            outputs = self.model(adv_images)

            # Calcul de la loss
            if self.targeted and target_labels is not None:
                # Attaque ciblée : minimiser la loss pour la classe cible
                loss = F.cross_entropy(outputs, target_labels)
                loss = -loss  # On veut minimiser, donc on prend l'opposé
            else:
                # Attaque non-ciblée : maximiser la loss pour la vraie classe
                loss = F.cross_entropy(outputs, labels)

            # Backward pass
            loss.backward()

            # Calcul du gradient
            grad_delta = delta.grad.detach()

            # Mise à jour selon la norme choisie
            if self.norm == "inf":
                delta = delta + self.alpha * grad_delta.sign()
                delta = self._project_linf(delta, self.eps)
            elif self.norm == "2":
                grad_norm = torch.norm(
                    grad_delta.view(grad_delta.shape[0], -1), dim=1, keepdim=True
                )
                grad_norm = grad_norm.view(-1, 1, 1, 1)
                grad_normalized = grad_delta / (grad_norm + 1e-8)
                delta = delta + self.alpha * grad_normalized
                delta = self._project_l2(delta, self.eps)
            elif self.norm == "1":
                grad_abs = torch.abs(grad_delta.view(grad_delta.shape[0], -1))
                grad_norm = torch.sum(grad_abs, dim=1, keepdim=True)
                grad_norm = grad_norm.view(-1, 1, 1, 1)
                grad_normalized = grad_delta / (grad_norm + 1e-8)
                delta = delta + self.alpha * grad_normalized
                delta = self._project_l1(delta, self.eps)

            # Projection dans [0,1] pour les images
            delta = torch.clamp(images + delta, 0, 1) - images
            delta = delta.detach()
            delta.requires_grad = True

        return images + delta

    def _random_init(self, images):
        """Initialisation aléatoire de la perturbation"""
        if self.norm == "inf":
            delta = torch.empty_like(images).uniform_(-self.eps, self.eps)
        elif self.norm == "2":
            delta = torch.randn_like(images)
            delta = delta.view(delta.shape[0], -1)
            delta = delta / torch.norm(delta, dim=1, keepdim=True)
            delta = delta.view(images.shape)
            delta = delta * self.eps * torch.rand(images.shape[0], 1, 1, 1)
        elif self.norm == "1":
            delta = torch.randn_like(images)
            delta = delta.view(delta.shape[0], -1)
            delta = delta / torch.norm(delta, p=1, dim=1, keepdim=True)
            delta = delta.view(images.shape)
            delta = delta * self.eps * torch.rand(images.shape[0], 1, 1, 1)

        return delta

    def _project_linf(self, delta, eps):
        """Projection L-infinite"""
        return torch.clamp(delta, -eps, eps)

    def _project_l2(self, delta, eps):
        """Projection L2"""
        delta_flat = delta.view(delta.shape[0], -1)
        delta_norm = torch.norm(delta_flat, dim=1, keepdim=True)
        delta_flat = delta_flat / torch.max(
            delta_norm / eps, torch.ones_like(delta_norm)
        )
        return delta_flat.view(delta.shape)

    def _project_l1(self, delta, eps):
        """Projection L1"""
        delta_flat = delta.view(delta.shape[0], -1)
        delta_abs = torch.abs(delta_flat)
        delta_norm = torch.sum(delta_abs, dim=1, keepdim=True)

        # Si la norme est déjà <= eps, pas de projection nécessaire
        mask = delta_norm <= eps

        # Sinon, on projette
        delta_flat = delta_flat / torch.max(
            delta_norm / eps, torch.ones_like(delta_norm)
        )

        return delta_flat.view(delta.shape)
